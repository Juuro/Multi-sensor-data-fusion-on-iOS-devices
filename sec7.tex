\chapter{Ausblick}
  \label{Ausblick}
Die naheliegendste Weiterentwicklungsmöglichkeit ist mit Sicherheit Augmented Reality. Das bedeutet, man verzichtet auf das vollständige 3D-Modell um die Bibliothek nachzubilden und nutzt stattdessen die Kamera des Geräts. Auf das Kamera-Bild kann man dann die Navigationspfeile, Markierungen und sonstige Informationen einzeichnen. Augmented Reality hat aber den Nachteil, dass die Genauigkeit nochmal deutlich höher sein muss, als wenn man mit einem 3D-Modell arbeitet. Denn Ungenauigkeiten fallen viel stärker auf, da der abgebildete Raum immer der reale Raum ist. Beim 3D-Modell ist es weniger auffällig wenn die zugrundeliegenden Berechnungen nicht ganz korrekt sind, weil man zwischen Navigation und Realität noch das 3D-Modell als Puffer hat. Das 3D-Modell ist dennoch nach wie vor nötig. Es wird wie bisher auch gebraucht, um die Position der Bücher in den Regalen zu vermerken und um die Wege zu berechnen. Es ist nur unsichtbar, das heißt es fällt auch der Wartungsaufwand bei sich ändernden Regalinhalten weg.

Die erforderliche Genauigkeit ist mit der hier angewendet Sensorfusion noch nicht am Ende ihrer Möglichkeiten. Der Kalman-Filter ist in der Lage, die Ungenauigkeit der Sensoren weiter herauszurechnen. Da die Genauigkeit der Position auch eines der größten Probleme bei der Berechnung der Orientierung ist, würde der Kalman-Filter auch bei der Positionsbestimmung hilfreich sein. Aufgrund der IMUs könnte mit Hilfe des Kalman-Filters auch eine Position bestimmt werden. Diese kann man mit den RSSI-Werten der Bluetooth-Sender und dem Partikelfilter fusionieren und so die Position weiter verbessern. Diese Berechnungen sind allerdings sehr schwierig, da, aufgrund der Geräte-Größe, die Sensoren immer merkbare Fehler produzieren werden. Es muss das richtige Mischungsverhältnis der verschiedenen Sensoren und Verfahren gefunden werden. Das beste Mischungsverhältnis kann aber zusätzlich auch von der aktuellen Art der Bewegung abhängig sein. Dies stellt eine zusätzliche Schwierigkeit dar. Die Entwicklung eines wirklich guten Kalman-Filters für Orientierungs- und Positionsdaten setzt eine große Menge theoretisches Vorwissen und vor allem Test-Zeit voraus. Dies wäre im Rahmen einer Studienarbeit leider nicht möglich gewesen.

Wenn das Kamerabild sowieso schon für Augmented Reality verwendet wird, könnte man die Positionsgenauigkeit zusätzlich durch Bilderkennung unterstützen. Ähnlich wie bei modernen Autos, die anhand der Fahrbahnmarkierung und der Leitplanken versuchen die Spur zu halten, könnten die Bilddaten mit einem internen 3D-Modell abgeglichen werden. So könnte geprüft werden, ob man sich wirklich gerade an der berechneten Stelle befindet. Wenn diese Überprüfung ständig stattfinden kann, kann sie sogar laufend in die Positions- und Orientierungsberechnung einfließen. Allerdings nur wenn es Sinn macht. Böden, Wände oder auch Regalansichten sehen oft sehr ähnlich aus, sodass keine verlässliche Aussage darüber getroffen werden kann wo man sich befindet.
Das Problem bei der Bildverarbeitung in Echtzeit ist der große Rechenaufwand. Es müsste ca. 20 mal in der Sekunde ein komplettes Bild analysiert werden. Das ist eine sehr große Herausforderung für die mobilen Prozessoren. Dennoch könnte auch eine weniger häufige Bildanalyse hilfreich sein. Es könnte dazu dienen die anderen Berechnungen zu verifizieren.

Um die Stabilität der Orientierungsberechnung weiter zu verbessern, könnte eine Magnetfeld-Störungs-Erkennung realisiert werden. Dazu müsste die Orientierung des Kompasses beobachtet werden und wenn eine, verglichen mit den Orientierungs-Daten der anderen Sensoren, unnatürliche Bewegung auftritt, eine Art Reaktions-Schwellenwert greifen. Das heißt, wenn der Kompass eine Drehung feststellt, aber das Gyroskop völlig ruhig bleibt, ist anzunehmen, dass eine Störung durch ein Magnetfeld vorliegt. Die genaue Umsetzung wird aber ebenfalls schwierig sein, denn die Fehler beider Sensoren dürfen nicht als unterschiedliche Drehung erkannt werden. Apple hat dieses Problem bereits selbst erkannt. Die API liefert auch schon deutlich stabilere Werte als zu Anfang.

Nützlich für die Navigation in Bibliotheken ansich könnte die NFC-Technik werden. Diese kleinen Chips sind schon heute keine Seltenheit in Bibliotheks-Büchern. Zum einen können sie zur Orientierung dienen. Über die gefundenen Bücher in der Umgebung kann der Standort relativ genau berechnet werden und als weiterer Fusionsinput für die Positionsbestimmung dienen. Zum anderen kann man mit NFC auch eine automatische Bibliotheks-Inventur durch normale Benutzer realisieren. Die App kann beim normalen Gang durch die Bibliothek die Bücher in der Umgebung erkennen und zumindest ihre ungefähre, auf ca. drei Meter genaue, Position bestimmen. Diese würde auf dem Bibliotheks-Server mit der korrekten Position abgeglichen. Wenn der Unterschied zu groß ist würden die Mitarbeiter der Bibliothek informiert. In der Zwischenzeit wird der neue, aber falsche Standort auf dem Server als aktuelle Position des Buches gespeichert, damit auch andere Bibliotheks-Besucher das Buch, trotz falscher Position, mit der Navigations-App finden können. Diese Technik ist bei wenigen Geräten bereits eingebaut. Allgemein steht sie aber, wenn man die Patent-Anmeldungen der Hersteller der letzten Jahre verfolgt, auf jeden Fall in den Startlöchern. \cite{wiki:006}

Um den Nutzerkreis einer solchen App zu vergrößern ist es auch empfehlenswert, die App für andere Geräte und Systeme neben iOS, wie zum Beispiel Android und Windows Phone,  zu entwickeln. Das Grundgerüst der App sollte dank Unity leicht auf andere Plattformen portierbar sein. Nur die API-Aufrufe müssen an das jeweilige System angepasst werden.